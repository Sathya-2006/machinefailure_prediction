{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24606,
     "status": "ok",
     "timestamp": 1753112059787,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "GIGiyhBjl8kf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'spark-3.4.1-bin-hadoop3.tgz'\n"
     ]
    }
   ],
   "source": [
    "# Install Java\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Download Spark 3.4.1 (fully supported)\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
    "\n",
    "# Extract Spark\n",
    "!tar -xzf spark-3.4.1-bin-hadoop3.tgz\n",
    "\n",
    "# Install PySpark helper\n",
    "!pip install -q findspark pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 7903,
     "status": "ok",
     "timestamp": 1753112070944,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "OUtoRmNunchR"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MachineFailurePrediction\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3172,
     "status": "ok",
     "timestamp": 1753112078605,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "A6Izvr3Jnz3z"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MachineFailurePrediction\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10538,
     "status": "ok",
     "timestamp": 1753112125808,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "qDqqM5lrn3Mo",
    "outputId": "9dd8a608-26a4-469b-a208-df4a29a14185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UDI: integer (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Air temperature [K]: double (nullable = true)\n",
      " |-- Process temperature [K]: double (nullable = true)\n",
      " |-- Rotational speed [rpm]: integer (nullable = true)\n",
      " |-- Torque [Nm]: double (nullable = true)\n",
      " |-- Tool wear [min]: integer (nullable = true)\n",
      " |-- Machine failure: integer (nullable = true)\n",
      " |-- TWF: integer (nullable = true)\n",
      " |-- HDF: integer (nullable = true)\n",
      " |-- PWF: integer (nullable = true)\n",
      " |-- OSF: integer (nullable = true)\n",
      " |-- RNF: integer (nullable = true)\n",
      "\n",
      "+---+----------+----+-------------------+-----------------------+----------------------+-----------+---------------+---------------+---+---+---+---+---+\n",
      "|UDI|Product ID|Type|Air temperature [K]|Process temperature [K]|Rotational speed [rpm]|Torque [Nm]|Tool wear [min]|Machine failure|TWF|HDF|PWF|OSF|RNF|\n",
      "+---+----------+----+-------------------+-----------------------+----------------------+-----------+---------------+---------------+---+---+---+---+---+\n",
      "|  1|    M14860|   M|              298.1|                  308.6|                  1551|       42.8|              0|              0|  0|  0|  0|  0|  0|\n",
      "|  2|    L47181|   L|              298.2|                  308.7|                  1408|       46.3|              3|              0|  0|  0|  0|  0|  0|\n",
      "|  3|    L47182|   L|              298.1|                  308.5|                  1498|       49.4|              5|              0|  0|  0|  0|  0|  0|\n",
      "|  4|    L47183|   L|              298.2|                  308.6|                  1433|       39.5|              7|              0|  0|  0|  0|  0|  0|\n",
      "|  5|    L47184|   L|              298.2|                  308.7|                  1408|       40.0|              9|              0|  0|  0|  0|  0|  0|\n",
      "+---+----------+----+-------------------+-----------------------+----------------------+-----------+---------------+---------------+---+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "df = spark.read.csv(\"/content/machine failure.csv\", header=True, inferSchema=True)\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 958,
     "status": "ok",
     "timestamp": 1753112143657,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "m29P5qQcoE4U"
   },
   "outputs": [],
   "source": [
    "#data cleaning\n",
    "# Drop rows with nulls\n",
    "df = df.dropna()\n",
    "\n",
    "# Optional: Treat outliers (Torque < 0 as example)\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\"Torque [Nm]\", when(df[\"Torque [Nm]\"] < 0, None).otherwise(df[\"Torque [Nm]\"]))\n",
    "df = df.na.fill(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3895,
     "status": "ok",
     "timestamp": 1753112800324,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "N5mKwGfeqjN6"
   },
   "outputs": [],
   "source": [
    "#outliers removal\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    q1, q3 = df.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "    IQR = q3 - q1\n",
    "    lower = q1 - 1.5 * IQR\n",
    "    upper = q3 + 1.5 * IQR\n",
    "    return df.filter((col(column) >= lower) & (col(column) <= upper))\n",
    "\n",
    "# Apply on selected numeric columns\n",
    "columns = [\n",
    "    \"Air temperature [K]\", \"Process temperature [K]\",\n",
    "    \"Rotational speed [rpm]\", \"Torque [Nm]\", \"Tool wear [min]\"\n",
    "]\n",
    "\n",
    "for col_name in columns:\n",
    "    df = remove_outliers(df, col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 679,
     "status": "ok",
     "timestamp": 1753112162914,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "XWrpDp15oHS0"
   },
   "outputs": [],
   "source": [
    "#stringindexer + onehotencoder\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"Type\", outputCol=\"Type_Indexed\")\n",
    "encoder = OneHotEncoder(inputCols=[\"Type_Indexed\"], outputCols=[\"Type_Encoded\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 607,
     "status": "ok",
     "timestamp": 1753112177318,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "TLqyQUoyoLRy"
   },
   "outputs": [],
   "source": [
    "#bucketizer\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), 40, 60, float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"Torque [Nm]\", outputCol=\"Torque_Bucket\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1753112195049,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "RJDC9rsioO1h"
   },
   "outputs": [],
   "source": [
    "#quantile discretizer\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=4, inputCol=\"Tool wear [min]\", outputCol=\"Wear_Discretized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1753112208657,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "TNryfiDHoU2D"
   },
   "outputs": [],
   "source": [
    "#transformers:vector assembler + scalers\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Air temperature [K]\", \"Process temperature [K]\", \"Rotational speed [rpm]\",\n",
    "               \"Torque [Nm]\", \"Torque_Bucket\", \"Wear_Discretized\"],\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1753112293644,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "oVxrSM-HolRK"
   },
   "outputs": [],
   "source": [
    "#window functions\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, row_number\n",
    "\n",
    "w = Window.partitionBy(\"Type\").orderBy(\"UDI\")\n",
    "df = df.withColumn(\"Rolling_Avg_Temp\", avg(\"Air temperature [K]\").over(w.rowsBetween(-2, 0)))\n",
    "df = df.withColumn(\"RowNum\", row_number().over(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1109,
     "status": "ok",
     "timestamp": 1753112320892,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "InpubSAWorp9"
   },
   "outputs": [],
   "source": [
    "#ml pipeline + model\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"Machine failure\", featuresCol=\"features\", numTrees=50)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    indexer, encoder, bucketizer, discretizer,\n",
    "    assembler, scaler, rf\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 21616,
     "status": "ok",
     "timestamp": 1753112357559,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "N4P4Hc1ZoySq"
   },
   "outputs": [],
   "source": [
    "#train-test split,train model\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "model = pipeline.fit(train)\n",
    "predictions = model.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4410,
     "status": "ok",
     "timestamp": 1753113032070,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "KFPzOYi4o2ya",
    "outputId": "d827d2ae-037a-43c5-c3e7-0ad9be60c8f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.9620\n",
      " Precision: 0.9578\n",
      " AUC: 0.9464\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Accuracy\n",
    "acc_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Machine failure\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = acc_eval.evaluate(predictions)\n",
    "\n",
    "# Precision\n",
    "precision_eval = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Machine failure\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = precision_eval.evaluate(predictions)\n",
    "\n",
    "# AUC (already used before)\n",
    "auc_eval = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Machine failure\", rawPredictionCol=\"rawPrediction\")\n",
    "auc = auc_eval.evaluate(predictions)\n",
    "\n",
    "print(f\" Accuracy: {accuracy:.4f}\")\n",
    "print(f\" Precision: {precision:.4f}\")\n",
    "print(f\" AUC: {auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1724,
     "status": "ok",
     "timestamp": 1753112420424,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "m1gXfnFopEv8",
    "outputId": "98520a41-59bb-425f-f078-afff72c5e0fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Type|        avg_torque|\n",
      "+----+------------------+\n",
      "|   M|40.017250583917296|\n",
      "|   L| 39.99659999999998|\n",
      "|   H| 39.83828514456634|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sql + temp views\n",
    "df.createOrReplaceTempView(\"machines\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT Type, AVG(`Torque [Nm]`) AS avg_torque\n",
    "    FROM machines\n",
    "    GROUP BY Type\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1779,
     "status": "ok",
     "timestamp": 1753112437249,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "Rbe3eQJdpKth"
   },
   "outputs": [],
   "source": [
    "#output sink\n",
    "predictions.select(\"Product ID\", \"Machine failure\", \"prediction\") \\\n",
    "    .write.mode(\"overwrite\").csv(\"/content/predictions_output.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40358,
     "status": "ok",
     "timestamp": 1753113782131,
     "user": {
      "displayName": "Harini Rajendran",
      "userId": "02661931798166405722"
     },
     "user_tz": -330
    },
    "id": "SK26FnmWrh5l",
    "outputId": "40e9b4ce-ce45-4ff7-edc5-2285f995a795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter air temperature [K]: 345\n",
      "Enter process temperature [K]: 355\n",
      "Enter rotational speed [rpm]: 2000\n",
      "Enter torque [Nm]: 75\n",
      "Enter tool wear [min]: 300\n",
      "Enter machine type (L/M/H): H\n",
      "+----------+--------------------+\n",
      "|prediction|         probability|\n",
      "+----------+--------------------+\n",
      "|       1.0|[0.29759767352565...|\n",
      "+----------+--------------------+\n",
      "\n",
      "\n",
      "âš ï¸ Predicted: Machine will FAIL (Confidence: 70.24%)\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ Take user input\n",
    "air_temp = float(input(\"Enter air temperature [K]: \"))\n",
    "process_temp = float(input(\"Enter process temperature [K]: \"))\n",
    "rpm = float(input(\"Enter rotational speed [rpm]: \"))\n",
    "torque = float(input(\"Enter torque [Nm]: \"))\n",
    "wear = float(input(\"Enter tool wear [min]: \"))\n",
    "m_type = input(\"Enter machine type (L/M/H): \")\n",
    "\n",
    "# ðŸ§¾ Create DataFrame from user input\n",
    "user_input = {\n",
    "    \"Air temperature [K]\": air_temp,\n",
    "    \"Process temperature [K]\": process_temp,\n",
    "    \"Rotational speed [rpm]\": rpm,\n",
    "    \"Torque [Nm]\": torque,\n",
    "    \"Tool wear [min]\": wear,\n",
    "    \"Type\": m_type\n",
    "}\n",
    "\n",
    "user_df = spark.createDataFrame([user_input])\n",
    "\n",
    "# âœ… Predict using the full trained pipeline\n",
    "user_prediction = model.transform(user_df)\n",
    "\n",
    "# ðŸ–¨ï¸ Show prediction and probabilities\n",
    "user_prediction.select(\"prediction\", \"probability\").show()\n",
    "\n",
    "# ðŸŽ¯ Friendly output\n",
    "pred_row = user_prediction.select(\"prediction\", \"probability\").collect()[0]\n",
    "pred = int(pred_row[\"prediction\"])\n",
    "probs = pred_row[\"probability\"]\n",
    "\n",
    "if pred == 1:\n",
    "    print(f\"\\nâš ï¸ Predicted: Machine will FAIL (Confidence: {probs[1]*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\nâœ… Predicted: Machine is SAFE (Confidence: {probs[0]*100:.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMsjrpS5n0uMieqBN6cxcLn",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
